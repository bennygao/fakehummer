= 在CenOS7系统上安装Kubernetes-1.13.3集群
:toc:
:numbered:
:source-highlighter: pygments

== 参考链接
https://www.codesheep.cn/2018/12/27/kubeadm-k8s1-13-1/[利用Kubeadm部署 Kubernetes 1.13.1集群实践录]

https://yq.aliyun.com/articles/626118[CentOS7中用kubeadm安装Kubernetes]


== 集群环境
在3个运行CentOS7系统的节点上安装k8s集群，1个节点作为Master，2个节点作为Minion。

.集群节点
|===
|IP地址 |hostname |身份

|192.168.9.91
|k8s-master
|Master

|192.168.9.95
|k8s-node-1
|Minion

|192.168.9.90
|k8s-node2
|Minion
|===

为了方便操作，可以在每个节点上设置ssh信任的证书，ssh登录节点时免去输密码的操作。
在sshd服务器上
[source,shell]
----
mkdir /root/.ssh
----

在ssh客户端
[source,sh]
----
scp ~/.ssh/id_rsa.pub root@<sshd_ip_addr>:/root/.ssh/authorized_keys
----

CentOS7设置hostname的命令为:
[source,sh]
----
hostnamectl set-hostname k8s-master
----

== 准备工作(所有节点)
=== 关闭CentOS系统防火墙
如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看Installing kubeadm中的 https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports[Check required ports] 一节。 这里简单起见在各节点禁用防火墙。
[source,sh]
----
systemctl disable firewalld.service
systemctl stop firewalld.service
----

[NOTE]
====
生产环境部署时，对于系统防火墙设置不能这么简单粗暴，需要跟系统环境部署的负责人具体设计防火墙规则。
====

=== 禁用SELINUX
[source,sh]
----
# 临时禁用
setenforce 0

# 永久禁用
vim /etc/selinux/config    # 或者修改/etc/sysconfig/selinux
SELINUX=disabled
----

=== 设置Pod依赖的网络参数
[source,sh]
----
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

modprobe bridge
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf
----

=== 关闭swap
先临时关闭
[source,sh]
----
swapoff -a
----

再修改 `/etc/fstab` 文件，注释掉 SWAP 的自动挂载（永久关闭swap，重启后也生效）。
[source,sh]
----
# 注释掉以下面这行
/dev/mapper/centos-swap swap                    swap    defaults        0 0
----

=== 安装Docker
==== 卸载老版本的Docker
如果有没有老版本Docker，则不需要这步。
[source,shell]
----
yum remove docker docker-common docker-selinux docker-engine
----

==== 安装Docker软件包
在 https://kubernetes.io/docs/setup/cri/#docker[kubernetes官网]有各OS下Docker的安装方法。
[source,shell]
----
# Install Docker CE
## Set up the repository
### Install required packages.
yum install yum-utils device-mapper-persistent-data lvm2

### Add docker repository.
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

## Install docker ce.
yum update && yum install docker-ce-18.06.2.ce

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart docker.
systemctl daemon-reload
systemctl restart docker
----

=== enable Docker service
[source,shell]
----
systemctl enable docker.service
----

==== 安装校验
[source,text]
----
[root@k8s-master ~]# docker version
Client:
 Version:           18.06.2-ce
 API version:       1.38
 Go version:        go1.10.3
 Git commit:        6d37f41
 Built:             Sun Feb 10 03:46:03 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server:
 Engine:
  Version:          18.06.2-ce
  API version:      1.38 (minimum version 1.12)
  Go version:       go1.10.3
  Git commit:       6d37f41
  Built:            Sun Feb 10 03:48:29 2019
  OS/Arch:          linux/amd64
  Experimental:     false
----

== 安装kubelet、kubeadm、kubectl(所有节点)
=== 配置阿里云的repo
google的repo被墙了，配置成阿里云的镜像。
[source,sh]
----
cat>>/etc/yum.repos.d/kubrenetes.repo<<EOF
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
EOF
----

=== 安装kubelet、kubeadm、kubectl软件包
[source,sh]
----
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
----

=== 获得kubectl版本号
[source,text]
----
[root@k8s-master ~]# kubectl version
Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
----
可以看到安装的kubectl版本号为 `v1.13.3`。

=== 安装kubernetes各组件的镜像
为了应对网络不畅通的问题，我们国内网络环境只能提前手动下载相关镜像并重新打 `tag`。
[source,sh]
----
docker pull mirrorgooglecontainers/kube-apiserver:v1.13.3
docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.3
docker pull mirrorgooglecontainers/kube-scheduler:v1.13.3
docker pull mirrorgooglecontainers/kube-proxy:v1.13.3
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.2.24
docker pull coredns/coredns:1.2.6
docker pull registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64

docker tag mirrorgooglecontainers/kube-apiserver:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3
docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3
docker tag mirrorgooglecontainers/kube-scheduler:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3
docker tag mirrorgooglecontainers/kube-proxy:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
docker tag registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64

docker rmi mirrorgooglecontainers/kube-apiserver:v1.13.3
docker rmi mirrorgooglecontainers/kube-controller-manager:v1.13.3
docker rmi mirrorgooglecontainers/kube-scheduler:v1.13.3
docker rmi mirrorgooglecontainers/kube-proxy:v1.13.3
docker rmi mirrorgooglecontainers/pause:3.1
docker rmi mirrorgooglecontainers/etcd:3.2.24
docker rmi coredns/coredns:1.2.6
docker rmi registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
----
[NOTE]
====
上面命令中的版本号 `v1.13.3` 都要用第一步中查出的kubectl版本号替换。
====

查看拉取到的images。
[source,text]
----
[root@k8s-master ~]# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.13.3             98db19758ad4        3 weeks ago         80.3MB
k8s.gcr.io/kube-apiserver            v1.13.3             fe242e556a99        3 weeks ago         181MB
k8s.gcr.io/kube-controller-manager   v1.13.3             0482f6400933        3 weeks ago         146MB
k8s.gcr.io/kube-scheduler            v1.13.3             3a6f709e97a0        3 weeks ago         79.6MB
k8s.gcr.io/coredns                   1.2.6               f59dcacceff4        3 months ago        40MB
k8s.gcr.io/etcd                      3.2.24              3cab8e1b9802        5 months ago        220MB
quay.io/coreos/flannel               v0.10.0-amd64       f0fad859c909        13 months ago       44.6MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        14 months ago       742kB
----

== 配置Master节点


=== 初始化Master
[source,text]
----
[root@k8s-master ~]# kubeadm init --kubernetes-version=v1.13.3 --apiserver-advertise-address 192.168.9.91 --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.13.3
[preflight] Running pre-flight checks
	[WARNING Hostname]: hostname "k8s-master" could not be reached
	[WARNING Hostname]: hostname "k8s-master": lookup k8s-master on 192.168.8.8:53: server misbehaving
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.9.91 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.9.91 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.9.91]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 20.005773 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "k8s-master" as an annotation
[mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: m4tzmd.tt8e9z1yepx1kdzw
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.9.91:6443 --token m4tzmd.tt8e9z1yepx1kdzw --discovery-token-ca-cert-hash sha256:0cf60cab0d24a0c39a5fedac0b87357f423054010a36dfa0e666c63dab7ae2f7
----
* `kubernetes-version`: 用于指定 k8s版本
* `apiserver-advertise-address`：用于指定使用 Master的哪个network interface进行通信，若不指定，则 kubeadm会自动选择具有默认网关的 interface
* `pod-network-cidr`：用于指定Pod的网络范围。该参数使用依赖于使用的网络方案，本文将使用经典的flannel网络方案。

=== 配置 kubectl
[source,sh]
----
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile
source /etc/profile
echo $KUBECONFIG
----

=== 安装Pod网络
安装 Pod网络是 Pod之间进行通信的必要条件，k8s支持众多网络方案，这里我们依然选用经典的 `flannel` 方案。

创建 `kube-flannel.yaml` 文件，内容如下：
[source,text]
----
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm64
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-arm64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-arm64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: arm
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-arm
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-arm
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-ppc64le
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: ppc64le
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-ppc64le
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-ppc64le
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-s390x
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: s390x
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-s390x
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-s390x
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
----

执行以下命令进行Pod网络初始化。
[source,sh]
----
kubectl apply -f kube-flannel.yaml
----

一旦 Pod网络安装完成，可以执行如下命令检查一下 CoreDNS Pod此刻是否正常运行起来了，一旦其正常运行起来，则可以继续后续步骤。
[source,text]
----
[root@k8s-master ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-86c58d9df4-lhpnx             1/1     Running   0          11m   10.244.0.3     k8s-master   <none>           <none>
kube-system   coredns-86c58d9df4-nldjp             1/1     Running   0          11m   10.244.0.2     k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          10m   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          10m   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          10m   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-wq7d4          1/1     Running   0          55s   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-proxy-vlnpx                     1/1     Running   0          11m   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          10m   192.168.9.91   k8s-master   <none>           <none>
----

同时我们可以看到主节点已经就绪 `kubectl get nodes`
[source,text]
----
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   13m   v1.13.3
----

使用 `kubectl get pod --all-namespaces -o wide` 确保所有的Pod都处于Running状态。
[source,text]
----
[root@k8s-master ~]# kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP             NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-86c58d9df4-lhpnx             1/1     Running   0          14m    10.244.0.3     k8s-master   <none>           <none>
kube-system   coredns-86c58d9df4-nldjp             1/1     Running   0          14m    10.244.0.2     k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          13m    192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          14m    192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          14m    192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-wq7d4          1/1     Running   0          4m5s   192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-proxy-vlnpx                     1/1     Running   0          14m    192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          13m    192.168.9.91   k8s-master   <none>           <none>
----

== 添加Minion集群节点
=== 在Master节点上创建永不过期的 `token`
参考 http://www.cnblogs.com/justmine/p/8886675.html[k8s踩坑记 - kubeadm join 之 token 失效]，以及 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/[kubeadm token]。

默认情况下，通过 `kubeadm create token` 创建的token，过期时间是 `24小时`，这就是为什么过了一天无法再次使用之前记录的 `kube join` 原生脚本的原因，可以运行 `kubeadm token create --ttl 0` 生成一个永不过期的token。

在Master节点上执行:
[source,text]
----
root@k8s-master ~]# kubeadm token create --ttl 0
go4637.x28gkcn67o74bbni

[root@k8s-master ~]# kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                   DESCRIPTION   EXTRA GROUPS
go4637.x28gkcn67o74bbni   <forever>   <never>   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token
----

获取ca证书sha256编码hash值
[source,text]
----
[root@k8s-master ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
d208bd3914178197deff369105670f5f0c211a556682201adf22700eeccc8966
----

=== Minion节点加入集群
在要加入集群的Minion节点上运行 `kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>` 加入集群。
[source,text]
----
[root@k8s-node-template ~]# kubeadm join --token go4637.x28gkcn67o74bbni k8s-master:6443 --discovery-token-ca-cert-hash sha256:d208bd3914178197deff369105670f5f0c211a556682201adf22700eeccc8966
[preflight] Running pre-flight checks
[discovery] Trying to connect to API Server "k8s-master:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://k8s-master:6443"
[discovery] Requesting info from "https://k8s-master:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "k8s-master:6443"
[discovery] Successfully established connection with API Server "k8s-master:6443"
[join] Reading configuration from the cluster...
[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "k8s-node-1" as an annotation

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
----

=== 验证结果
==== 查看节点状态
[source,text]
----
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   10m     v1.13.3
k8s-node-1   Ready    <none>   5m46s   v1.13.3
----
可以看到master和新加入的node节点都处于 `Ready` 状态了。

==== 查看所有 Pod状态
[source,text]
----
[root@k8s-master ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-86c58d9df4-qjspk             1/1     Running   0          11m     10.244.1.2     k8s-node-1   <none>           <none>
kube-system   coredns-86c58d9df4-zbj2g             1/1     Running   0          11m     10.244.1.3     k8s-node-1   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          10m     192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          10m     192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          10m     192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-blwh8          1/1     Running   0          6m6s    192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-db4qk          1/1     Running   0          6m6s    192.168.9.95   k8s-node-1   <none>           <none>
kube-system   kube-proxy-f52ks                     1/1     Running   0          11m     192.168.9.91   k8s-master   <none>           <none>
kube-system   kube-proxy-zxk4g                     1/1     Running   0          7m26s   192.168.9.95   k8s-node-1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          11m     192.168.9.91   k8s-master   <none>           <none>
----

== 从集群中拆除节点
=== 在Master节点上删除Node
[source,sh]
----
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
----

[source,text]
----
[root@k8s-master ~]# kubectl drain k8s-node-1 --delete-local-data --force --ignore-daemonsets
node/k8s-node-1 cordoned
WARNING: Ignoring DaemonSet-managed pods: kube-flannel-ds-amd64-db4qk, kube-proxy-zxk4g
pod/coredns-86c58d9df4-qjspk evicted
pod/coredns-86c58d9df4-zbj2g evicted
node/k8s-node-1 evicted

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS                     ROLES    AGE   VERSION
k8s-master   Ready                      master   15m   v1.13.3
k8s-node-1   Ready,SchedulingDisabled   <none>   10m   v1.13.3

[root@k8s-master ~]# kubectl delete node k8s-node-1
node "k8s-node-1" deleted

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   15m   v1.13.3
----

=== 在被删除的节点上reset kubeadm配置
在被删除的Node节点上执行 `kubeadm reset` 重置配置。
[source,text]
----
[root@k8s-node-template ~]# kubeadm reset
[reset] WARNING: changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] are you sure you want to proceed? [y/N]: y
[preflight] running pre-flight checks
[reset] no etcd config found. Assuming external etcd
[reset] please manually reset etcd to prevent further issues
[reset] stopping the kubelet service
[reset] unmounting mounted directories in "/var/lib/kubelet"
[reset] deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually.
For example:
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.
----

=== 拆除整个集群
在Master节点上运行 `kubeadm reset` 可以拆除整个集群，并重置所有集群配置。集群拆除后，需要从 `kubeadm init` 命令开始重新初始化集群Master，并逐一加入各个Node。

== kubernetes Web UI (Dashboard)
=== 安装kubernetes dashboard
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/[kubernetes dashboard]是kubernetes官方提供的一个基于Web UI的集群监控管理工具。kubernets的缺省安装中没有包括dashboard组件，需要运行以下命令安装。
[source,sh]
----
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
----

=== 卸载kubernetes dashboard
[source,sh]
----
kubectl get secret,sa,role,rolebinding,services,deployments --namespace=kube-system | grep dashboard
----
依次用 `kubectl delete` 命令删除列出的资源。
